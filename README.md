# VLM-clip-flicker_30k-dataet-
Vision-Language Retrieval with CLIP, FAISS, and Streamlit  This project showcases a lightweight Vision-Language Retrieval system built using the CLIP model. It leverages the Flickr30k dataset to encode both images and captions into a shared embedding space, enabling seamless cross-modal search. By indexing the image features with FAISS.
